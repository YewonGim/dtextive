{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import enchant\n",
    "from guess_language import guess_language\n",
    "import re\n",
    "import pickle\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "files=os.listdir('Lang_Eng/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_datafile(files):\n",
    "    raw_text=open('Lang_Eng/'+files,'r')\n",
    "    \n",
    "    #Remove UTF-8 Char\n",
    "    messy_list=[]\n",
    "    for item in raw_text:\n",
    "        messy_list.append(re.sub(r'(\\\\+([A-z]|[0-9]){3})','',item))\n",
    "    \n",
    "    #Prep the sentences for tokenization\n",
    "    Eng_sent_prep=[]\n",
    "    for item in messy_list:\n",
    "        Eng_sent_prep.append((re.sub(r'(.!?)([A-Z]+)', r'\\1 \\2', item)))\n",
    "    \n",
    "    #Sentence tokenization\n",
    "    Eng_sent=[]\n",
    "    for line in Eng_sent_prep:\n",
    "        sent_tokenize_list = sent_tokenize(line)\n",
    "        Eng_sent.append(sent_tokenize_list)\n",
    "    flatten_sent = [val for sublist in Eng_sent for val in sublist]\n",
    "    \n",
    "    #Remove non-english sentences\n",
    "    Eng_only=[]\n",
    "    for sent in flatten_sent:\n",
    "        if guess_language(sent) =='en':\n",
    "            Eng_only.append(sent)\n",
    "            \n",
    "    #Create word tokenized list\n",
    "    Eng_word_ml=[]\n",
    "    for item in Eng_only:\n",
    "        dsent=[]\n",
    "        words= word_tokenize(item)\n",
    "        for item in words:\n",
    "            if item.isalnum() == False:\n",
    "                pass\n",
    "            else:\n",
    "                dsent.append(item.lower())\n",
    "        if len(dsent)>1:\n",
    "            Eng_word_ml.append(dsent)\n",
    "            \n",
    "    #Remove random char\n",
    "    Eng_word_ml_cleaned=[]\n",
    "    for sent in Eng_word_ml:\n",
    "        dsent=[]\n",
    "        if len(sent)>2:\n",
    "            for word in sent:\n",
    "                if word in 'bcdfghjklmnopqrstvwxz':\n",
    "                    pass\n",
    "                else:\n",
    "                    dsent.append(word)\n",
    "            Eng_word_ml_cleaned.append(dsent)    \n",
    "            \n",
    "    return Eng_word_ml_cleaned        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_words=[]\n",
    "for file in files:\n",
    "    words=[]\n",
    "    words=clean_datafile(file)\n",
    "    total_words.append(words)\n",
    "\n",
    "flatten_words = [val for sublist in total_words for val in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69728"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(flatten_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pd_frame_maker(sentlist, label):\n",
    "    df=pd.DataFrame({'sent': sentlist, 'label': label})\n",
    "    df['sentence']=df['sent'].apply(lambda x: \" \".join(x))\n",
    "    df['length']=df['sent'].apply(lambda x: len(x))\n",
    "    df=df[df['length']>1]\n",
    "    df['sentence']=df['sentence'].drop_duplicates()\n",
    "    df['language']=df['sentence'].apply(lambda x: guess_language(str(x)))\n",
    "    df=df[df['language']=='en']\n",
    "    df = df.dropna()   \n",
    "    df = df.ix[:,:4]\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=pd_frame_maker(flatten_words,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_pickle('data/lang8_eng.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:insight]",
   "language": "python",
   "name": "conda-env-insight-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
